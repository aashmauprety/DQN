{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled1.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "il72brFsJQBS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import gym\n",
        "from dqn_tf import DeepQNetwork, Agent\n",
        "from utils import plotLearning\n",
        "import numpy as np\n",
        "from gym import wrappers\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def preprocess(observation):\n",
        "    observation = observation / 255\n",
        "    return np.mean(observation[30:,:], axis=2).reshape(180,160,1)\n",
        "\n",
        "def stack_frames(stacked_frames, frame, buffer_size):\n",
        "    if stacked_frames is None:\n",
        "        stacked_frames = np.zeros((buffer_size, *frame.shape))\n",
        "        for idx, _ in enumerate(stacked_frames):\n",
        "            stacked_frames[idx,:] = frame\n",
        "    else:\n",
        "        stacked_frames[0:buffer_size-1,:] = stacked_frames[1:,:]\n",
        "        stacked_frames[buffer_size-1, :] = frame\n",
        "\n",
        "    stacked_frames = stacked_frames.reshape(1, *frame.shape[0:2], buffer_size)\n",
        "\n",
        "    return stacked_frames\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    #os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
        "    #os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
        "    #os.environ['TF_FORCE_GPU_ALLOW_GROWTH'] = 'true'\n",
        "\n",
        "    env = gym.make('Breakout-v0')\n",
        "    load_checkpoint = False\n",
        "    agent = Agent(gamma=0.99, epsilon=1.0, alpha=0.000025, input_dims=(180,160,4),\n",
        "                  n_actions=3, mem_size=25000, batch_size=64)\n",
        "    if load_checkpoint:\n",
        "        agent.load_models()\n",
        "    filename = 'breakout-alpha0p000025-gamma0p9-only-one-fc-2.png'\n",
        "    scores = []\n",
        "    eps_history = []\n",
        "    numGames = 50000\n",
        "    stack_size = 4\n",
        "    score = 0\n",
        "    # uncomment the line below to record every episode.\n",
        "    #env = wrappers.Monitor(env, \"tmp/breakout-0\",\n",
        "    #                         video_callable=lambda episode_id: True, force=True)\n",
        "    \"\"\"\n",
        "    print(\"Loading up the agent's memory with random gameplay\")\n",
        "    while agent.mem_cntr < 25000:\n",
        "        done = False\n",
        "        observation = env.reset()\n",
        "        observation = preprocess(observation)\n",
        "        stacked_frames = None\n",
        "        observation = stack_frames(stacked_frames, observation, stack_size)\n",
        "        while not done:\n",
        "            action = np.random.choice([0, 1, 2])\n",
        "            action += 1\n",
        "            observation_, reward, done, info = env.step(action)\n",
        "            observation_ = stack_frames(stacked_frames,\n",
        "                                        preprocess(observation_), stack_size)\n",
        "            action -= 1\n",
        "            agent.store_transition(observation, action,\n",
        "                                   reward, observation_, int(done))\n",
        "            observation = observation_\n",
        "    print(\"Done with random gameplay. Game on.\")\n",
        "    \"\"\"\n",
        "    n_steps = 0\n",
        "    for i in range(numGames):\n",
        "        done = False\n",
        "        #if i % 100 == 0 and i > 0:\n",
        "        #    x = [j+1 for j in range(i)]\n",
        "\n",
        "        #    plotLearning(x, scores, eps_history, filename)\n",
        "        observation = env.reset()\n",
        "        observation = preprocess(observation)\n",
        "        stacked_frames = None\n",
        "        observation = stack_frames(stacked_frames, observation, stack_size)\n",
        "        score = 0\n",
        "        while not done:\n",
        "            action = agent.choose_action(observation)\n",
        "            action += 1\n",
        "            observation_, reward, done, info = env.step(action)\n",
        "            n_steps += 1\n",
        "            observation_ = stack_frames(stacked_frames,\n",
        "                                        preprocess(observation_), stack_size)\n",
        "            score += reward\n",
        "            action -= 1\n",
        "            agent.store_transition(observation, action,\n",
        "                                   reward, observation_, int(done))\n",
        "            observation = observation_\n",
        "            if n_steps % 4 == 0:\n",
        "                agent.learn()\n",
        "        if i % 12 == 0 and i > 0:\n",
        "            avg_score = np.mean(scores[max(0, i-12):(i+1)])\n",
        "            print('episode: ', i,'score: ', score,\n",
        "                 ' average score %.3f' % avg_score,\n",
        "                'epsilon %.3f' % agent.epsilon)\n",
        "            agent.save_models()\n",
        "        else:\n",
        "            print('episode: ', i,'score: ', score)\n",
        "        eps_history.append(agent.epsilon)\n",
        "        scores.append(score)\n",
        "    x = [i+1 for i in range(numGames)]\n",
        "    plotLearning(x, scores, eps_history, filename)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}